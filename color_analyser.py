# -*- coding: utf-8 -*-
"""Sem 6 Mini Project - Colour Analysis Tool.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Wrd7GgTbL2JWQOO6MswbHCFPHM2Qag2
"""

!pip install mediapipe

!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task

from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np
import matplotlib.pyplot as plt


def draw_landmarks_on_image(rgb_image, detection_result):
  face_landmarks_list = detection_result.face_landmarks
  annotated_image = np.copy(rgb_image)

  # Loop through the detected faces to visualize.
  for idx in range(len(face_landmarks_list)):
    face_landmarks = face_landmarks_list[idx]

    # Draw the face landmarks.
    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
    face_landmarks_proto.landmark.extend([
      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks
    ])

    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,
        landmark_drawing_spec=None,
        connection_drawing_spec=mp.solutions.drawing_styles
        .get_default_face_mesh_tesselation_style())
    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,
        landmark_drawing_spec=None,
        connection_drawing_spec=mp.solutions.drawing_styles
        .get_default_face_mesh_contours_style())
    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_IRISES,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp.solutions.drawing_styles
          .get_default_face_mesh_iris_connections_style())

  return annotated_image

def plot_face_blendshapes_bar_graph(face_blendshapes):
  # Extract the face blendshapes category names and scores.
  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]
  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]
  # The blendshapes are ordered in decreasing score value.
  face_blendshapes_ranks = range(len(face_blendshapes_names))

  fig, ax = plt.subplots(figsize=(12, 12))
  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])
  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)
  ax.invert_yaxis()

  # Label each bar with values
  for score, patch in zip(face_blendshapes_scores, bar.patches):
    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f"{score:.4f}", va="top")

  ax.set_xlabel('Score')
  ax.set_title("Face Blendshapes")
  plt.tight_layout()
  plt.show()

from google.colab import files
uploaded = files.upload()

for filename in uploaded:
   content = uploaded[filename]
   with open(filename, 'wb') as f:
     f.write(content)

if len(uploaded.keys()):
   IMAGE_FILE = next(iter(uploaded))
   print('Uploaded file:', IMAGE_FILE)

import cv2
from google.colab.patches import cv2_imshow
import math

# Height and width that will be used by the model
DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480

# Performs resizing and showing the image
def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


# Preview the image(s)
images = {name: cv2.imread(name) for name in uploaded}
for name, image in images.items():
  print(name)
  resize_and_show(image)

# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an FaceLandmarker object.
base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')
options = vision.FaceLandmarkerOptions(base_options=base_options,
                                       output_face_blendshapes=True,
                                       output_facial_transformation_matrixes=True,
                                       num_faces=1)
detector = vision.FaceLandmarker.create_from_options(options)

# STEP 3: Load the input image.
for i in images:
  image = mp.Image.create_from_file(i)

  # STEP 4: Detect face landmarks from the input image.
  detection_result = detector.detect(image)

  # STEP 5: Process the detection result. In this case, visualize it.
  annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)
  cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))

"""## **Extracting eye colour**"""

def main():
    # Initialize MediaPipe Face Mesh
    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)

    # Initialize the dictionary to store eye colors
    EC = {}

    for i in images:
        # Load the image
        image_path = i
        image = cv2.imread(image_path)

        if image is None:
            print("Error: Could not read image from", image_path)
            return

        # Convert image to RGB format (MediaPipe works with RGB)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Process the image with Face Mesh
        results = mp_face_mesh.process(image_rgb)

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Convert landmarks to pixel coordinates
                height, width, _ = image.shape
                left_eye_x = int(face_landmarks.landmark[159].x * width)
                left_eye_y = int(face_landmarks.landmark[159].y * height)
                right_eye_x = int(face_landmarks.landmark[386].x * width)
                right_eye_y = int(face_landmarks.landmark[387].y * height)

                # Extract eye regions with a margin for better accuracy
                eye_region_margin = 20
                left_eye_region = image[left_eye_y - eye_region_margin:left_eye_y + eye_region_margin,
                                        left_eye_x - eye_region_margin:left_eye_x + eye_region_margin]
                right_eye_region = image[right_eye_y - eye_region_margin:right_eye_y + eye_region_margin,
                                         right_eye_x - eye_region_margin:right_eye_x + eye_region_margin]

                # Extract eye color (handle potential empty regions)
                eye_colors = extract_eye_color(image, left_eye_region, right_eye_region)
                if eye_colors is None:
                    print("Warning: Could not extract eye color (possibly due to invalid eye regions).")
                    continue

                #left_eye_color, right_eye_color = eye_colors
                left_eye_color_mode = st.mode(left_eye_region, axis=(0, 1)).mode[0]
                right_eye_color_mode = st.mode(right_eye_region, axis=(0, 1)).mode[0]

                # Example: Find color names
                #left_eye_color_name = find_color(tuple(left_eye_color.astype(int)))
                #right_eye_color_name = find_color(tuple(right_eye_color.astype(int)))
                left_eye_color_name = find_color(left_eye_color_mode)
                right_eye_color_name = find_color(right_eye_color_mode)

                EC[i] = left_eye_color_mode

                # Display eye color in BGR format
                print("Left Eye Color (BGR):", left_eye_color_mode, left_eye_color_name)
                print("Right Eye Color (BGR):", right_eye_color_mode, right_eye_color_name)

                # Draw rectangles around eye regions for visualization
                cv2.rectangle(image, (left_eye_x - eye_region_margin, left_eye_y - eye_region_margin),
                              (left_eye_x + eye_region_margin, left_eye_y + eye_region_margin), (0, 255, 0), 2)
                cv2.rectangle(image, (right_eye_x - eye_region_margin, right_eye_y - eye_region_margin),
                              (right_eye_x + eye_region_margin, right_eye_y + eye_region_margin), (0, 255, 0), 2)

            # Display the annotated image with eye regions
            cv2_imshow(image)
            cv2.waitKey(0)
            cv2.destroyAllWindows()

        else:
            print("No face detected.")

if __name__ == "__main__":
    main()

import numpy as np
import statistics as st

def main():
    # Initialize MediaPipe Face Mesh
    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)

    # Initialize the dictionary to store eye colors
    EC = {}

    for i in images:
        # Load the image
        image_path = i
        image = cv2.imread(image_path)

        if image is None:
            print("Error: Could not read image from", image_path)
            return

        # Convert image to RGB format (MediaPipe works with RGB)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Process the image with Face Mesh
        results = mp_face_mesh.process(image_rgb)

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Convert landmarks to pixel coordinates
                height, width, _ = image.shape
                left_eye_x = int(face_landmarks.landmark[159].x * width)
                left_eye_y = int(face_landmarks.landmark[159].y * height)

                # Extract eye regions with a margin for better accuracy
                eye_region_margin = 20
                left_eye_region = image[left_eye_y - eye_region_margin:left_eye_y + eye_region_margin,
                                        left_eye_x - eye_region_margin:left_eye_x + eye_region_margin]



                # Store the mode in EC dictionary for this image as NumPy array
                if i not in EC:
                    EC[i] = [np.array(eye_color_mode)]
                else:
                    EC[i].append(np.array(eye_color_mode))

                # Display eye color in BGR format
                print("Left Eye Color (BGR):", eye_color_mode)

                # Draw rectangles around eye regions for visualization
                cv2.rectangle(image, (left_eye_x - eye_region_margin, left_eye_y - eye_region_margin),
                              (left_eye_x + eye_region_margin, left_eye_y + eye_region_margin), (0, 255, 0), 2)

            # Display the annotated image with eye regions
            cv2_imshow(image)
            cv2.waitKey(0)
            cv2.destroyAllWindows()

        else:
            print("No face detected.")

if __name__ == "__main__":
    main()

"""## **Extracting skin colour**"""

import cv2
import mediapipe as mp
import numpy as np
from sklearn.cluster import KMeans

def extract_skin_color(image):
    # Initialize MediaPipe Face Detection
    mp_face_detection = mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5)

    # Convert image to RGB format (MediaPipe works with RGB)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Detect faces in the image
    results = mp_face_detection.process(image_rgb)

    # Check if any faces are detected
    if results.detections:
        for detection in results.detections:
            # Get the bounding box of the face
            bboxC = detection.location_data.relative_bounding_box
            ih, iw, _ = image.shape
            bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)

            # Extract the facial region containing the skin area
            face_region = image[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]

            # Reshape the face region for clustering
            pixels = face_region.reshape((-1, 3))

            # Perform K-means clustering to find dominant colors
            kmeans = KMeans(n_clusters=3)
            kmeans.fit(pixels)
            dominant_colors = kmeans.cluster_centers_.astype(int)

            # Get the most dominant color (assumed to be skin color)
            skin_color = dominant_colors[np.argmax(np.sum(dominant_colors, axis=1))]

            return bbox, skin_color

    else:
        print("No face detected in the image.")
        return None, None

SC = {}
# Load the input image
for i in images:
  image_path = i
  image = cv2.imread(image_path)

  if image is None:
      print("Error: Could not read image from", image_path)
  else:
      # Extract bounding box and dominant skin color
      bbox, skin_color = extract_skin_color(image)

      if skin_color is not None:
          # Draw rectangle around the face region
          cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]), (0, 255, 0), 2)

          # Create a blank image filled with the skin color
          color_image = np.full((100, 100, 3), skin_color, dtype=np.uint8)

          # Display the original image with the selected face region
          cv2_imshow(image)

          # Display the skin color
          cv2_imshow(color_image)

          print("Dominant skin color (BGR):", skin_color)
          SC[i] = skin_color
          cv2.waitKey(0)
          cv2.destroyAllWindows()

      else:
          print("Failed to extract skin color.")

"""## **Extracting hair colour**"""

import numpy as np
import mediapipe as mp

from mediapipe.tasks import python
from mediapipe.tasks.python import vision


BG_COLOR = (192, 192, 192) # gray
MASK_COLOR = (255, 255, 255) # white


# Create the options that will be used for ImageSegmenter
base_options = python.BaseOptions(model_asset_path='hair_segmenter.tflite')
options = vision.ImageSegmenterOptions(base_options=base_options,
                                       output_category_mask=True)

# Create the image segmenter
with vision.ImageSegmenter.create_from_options(options) as segmenter:

  # Loop through demo image(s)
  for image_file_name in images:

    # Create the MediaPipe image file that will be segmented
    image = mp.Image.create_from_file(image_file_name)

    # Retrieve the masks for the segmented image
    segmentation_result = segmenter.segment(image)
    category_mask = segmentation_result.category_mask

    # Generate solid color images for showing the output segmentation mask.
    image_data = image.numpy_view()
    fg_image = np.zeros(image_data.shape, dtype=np.uint8)
    fg_image[:] = MASK_COLOR
    bg_image = np.zeros(image_data.shape, dtype=np.uint8)
    bg_image[:] = BG_COLOR

    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.2
    output_image = np.where(condition, fg_image, bg_image)


    print(f'Segmentation mask of {name}:')
    resize_and_show(output_image)

import cv2
import numpy as np

# Create the segmenter
with python.vision.ImageSegmenter.create_from_options(options) as segmenter:
    HC = {}
    # Loop through available image(s)
    for image_file_name in uploaded:

        # Create the MediaPipe Image
        image = mp.Image.create_from_file(image_file_name)

        # Retrieve the category masks for the image
        segmentation_result = segmenter.segment(image)
        category_mask = segmentation_result.category_mask.numpy_view()

        # Extract pixels corresponding to category mask 1 (assuming hair category)
        hair_pixels = image.numpy_view()[category_mask == 1]

        # Check if there are any hair pixels
        if len(hair_pixels) == 0:
            print(f"No hair pixels found in {image_file_name}.")
        else:
            # Calculate the dominant color in hair pixels
            hair_color_bgr = np.uint8(np.mean(hair_pixels, axis=0))
            HC[image_file_name] = hair_color_bgr
            print(f"Dominant Hair Color (BGR) in {image_file_name}: {tuple(hair_color_bgr)}")

            # Find contours of the hair region
            contours, _ = cv2.findContours(category_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            # Draw contours on the original image
            image_with_contours = cv2.drawContours(cv2.cvtColor(image.numpy_view(), cv2.COLOR_RGB2BGR), contours, -1, (0, 255, 0), 2)



            # Display the image with the outlined hair region
            cv2_imshow(image_with_contours)
            # Display the hair surrounding color
            cv2_imshow(color_image)
            print("Dominant hair surrounding color (BGR):", hair_color_bgr)
            cv2.waitKey(0)
            cv2.destroyAllWindows()

print(HC)

def classify_season(bgr_color):
    # Define color ranges for each season in BGR format
    WINTER_RANGE = [(0, 0, 100), (50, 50, 255)]  # Winter color range
    SPRING_RANGE = [(0, 128, 0), (255, 255, 128)]  # Spring color range
    SUMMER_RANGE = [(0, 100, 0), (255, 255, 255)]  # Summer color range
    AUTUMN_RANGE = [(0, 0, 0), (255, 128, 128)]  # Autumn color range

    # Check which season the color belongs to
    if in_color_range(bgr_color, WINTER_RANGE):
        return "Winter"
    elif in_color_range(bgr_color, SPRING_RANGE):
        return "Spring"
    elif in_color_range(bgr_color, SUMMER_RANGE):
        return "Summer"
    elif in_color_range(bgr_color, AUTUMN_RANGE):
        return "Autumn"
    else:
        return "Unknown"

def in_color_range(color, color_range):
    # Check if the given color falls within the specified color range
    return all(color_range[0][i] <= color[i] <= color_range[1][i] for i in range(3))


for i in uploaded:
    # Example BGR values of eye, skin, and hair colors
    eye_color_bgr = EC[i]  # eye color in BGR format
    hair_color_bgr = HC[i]  # hair color in BGR format
    skin_color_bgr = SC[i]  # skin color in BGR format
    print(i)
    # Classify eye color
    eye_season = classify_season(eye_color_bgr)
    print("Eye Color Season:", eye_season)

    # Classify hair color
    hair_season = classify_season(hair_color_bgr)
    print("Hair Color Season:", hair_season)

    # Classify skin color
    skin_season = classify_season(skin_color_bgr)
    print("Skin Color Season:", skin_season)



